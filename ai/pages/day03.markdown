status: hidden
Template: waiting
odate: 2019-02-06T09:00:00
Title: Jour 03 - D√©couverte de Tensorflow

‚ö† **Attention** ‚ö†

N'oubliez pas que vous n'√™tes pas not√©, prenez le temps de comprendre correctement chaque exercices, Tensorflow sera votre outil de travail pour la suite de la piscine.

Les exercices de cette journ√©e sont tr√®s peu guid√©s volontairement, nous souhaitons vous apprendre comment se documenter seul dans un milleu scientifique et en constante √©volution.

PS: Evitez quand m√™me de copier le code sur internet sinon c'est NO FUN. 

PS PS: Vraiment

# Introduction

Tensorflow est une librairie open-source tr√®s puissante d√©velopp√©e par Google Brain pour le d√©veloppement des **Deep Neural Networks** (DNNs). La librairie a √©t√© dans un premier temps disponible en novembre 2015 sous license Apache 2.x, le github du projet poss√®de plus de 800 contributeurs pour + de 17.000 de commits en seulement 2 ans. 

Voyons dans un premier temps pourquoi Tensorflow s'illustre comme le Framework n¬∞1 dans le d√©veloppement de DNN. Tensorflow permet le d√©ploiement grandement facilit√© sur plusieurs CPU, plusieurs GPU, dans un serveur, un smartphone, un laptop. Il y a √©normement de librairies pour le d√©veloppement de DNN aujourd'hui (PyTorch, Theano, Caffe, MxNet)

La plupart de ces librairies poss√®de un syst√®me d'auto-d√©rivation (**auto-differentation** in english my friend), elles supportent aussi la r√©partition sur CPU / GPU et ont la plupart des mod√®les connus directement impl√©ment√©s (CNNs, RNNs, DBNs).

Vous vous demandez surement ce qui rend Tensorflow sp√©cial ?

- Tensorflow fonctionne avec beaucoup de languages (Python, C++, Java, R, Go, etc..) et m√™me la plupart des languages qui sorte encore aujourd'hui propose des "Bindings" avec l'API C++ de Tensorflow
- Tensorflow fonctionne sur plusieurs plateformes (Mobile, Distribu√©)
- Tensorflow est support√© par tout les **cloud providers** (AWS, Google, Azure)
- Keras une API **haut niveau** tr√®s connue a √©t√© int√©gr√©e avec Tensorflow et le rend encore plus simple d'utilisation 
- Tensorflow facilite √©normement la mise en production 
- Tensorflow poss√®de une tr√®s bonne communaut√© pour le support
- Tensorflow est plus qu'un framework, c'est une suite, compos√©e de Tensorflow, Tensorboard, TensorServing

---

## Installation

> Nous travaillerons exlusivement sous python 3.x, merci de ne pas utiliser python 2 afin d'√©viter des questions inutiles.

L'installation de Tensorflow est relativement simple, elle se fait de cette mani√®re:

```shell
pip3 install tensorflow --user
```

Vous pouvez v√©rifier votre installation Tensorflow en tapant ces quelques lignes dans votre interpreteur python3:

```bash
python3
```

Puis tapez:

```python
import tensorflow as tf
print(tf.__version__)
```

Si vous avez rencontr√© un probl√®me lors de l'installation et que vous avez trouv√© la solution, merci de la communiquer sur le chat de la piscine.

---

## Graph Tensorflow

Tensorflow repr√©sente tout les calculs qui seront effectu√©s sous forme de [graph](https://en.wikipedia.org/wiki/Graph_theory) de calcul.

Chaque noeuds de ce graph est nomm√© [**Tensor**](https://www.tensorflow.org/guide/tensors), en math√©matiques un Tenseur est un concept qui permet de g√©n√©raliser: les scalaires, les matrices, les vecteurs et les donn√©es ayant des dimensions encore plus √©lev√©es.

Tensorflow √† choisi de repr√©senter les noeuds par des tenseurs car chaque noeuds du graph repr√©sente enfaite une op√©ration qui produira √©ventuellement une matrice, un vecteur, un scalaire ou m√™me rien du tout.


<img style="width: 350px" src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0901.png"></img>

Ce type de graph ce lit du bas vers le haut: 
 
1. On multiplie $x$ par lui m√™me pour $x \longrightarrow x¬≤$. 
2. L'√©tape suivante est de multiplier le r√©sultat de **l'√©tape 1** par $y$. 
3. On prend ensuite la variable $y$ et on l'additionne √† la constante $2$. 
4. Ensuite on prend les r√©sultats de ces deux op√©rations et on les additionnent. 

Cela constitue un moyen simple et tr√®s clair de repr√©senter une fonction.
Mais Google a √©t√© encore plus loin en d√©veloppant Tensorflow, grace √† ce principe de graph il est √©galement possible de r√©partir tr√®s facilement les calculs.


<img style="width: 350px" src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0902.png"></img>

Sur cette image on voit qu'il est possible de d√©couper notre graph en plusieurs blocs qui sont ind√©pendants l'un de l'autre et c'est comme sa que Tensorflow g√®re comme un grand la r√©partition des calculs sur GPU.

Il est important de noter qu'un graph Tensorflow d√©crit uniquement les op√©rations √† effectuer, aucune variables ne sont encore initialis√©es a la cr√©ation du Graph.
  
---

## Session Tensorflow

Une session Tensorflow (```tf.Session```) permet de charger un graph de calcul et d'executer les tenseurs individuellement, elle s'occupe aussi d'initialiser les variables.

Il est possible d'avoir plusieurs sessions qui executent le m√™me graph de calculs mais avec des variables diff√©rentes.

Un exemple d'execution de graph dans une session:

<img src="https://www.tensorflow.org/images/tensors_flowing.gif"></img>

---

## Hello world dans Tensorflow

Le premier programme que vous allez cr√©er en utilisant Tensorflow va simplement afficher un message sur votre Terminal.

Dans votre interpr√©teur python3 commencer par importer Tensorflow:

```python
import tensorflow as tf
```

On cr√©er un tenseur qui repr√©sente une variable **constante** (```tf.constant```), dans notre cas il s'agira d'une chaine de caract√®res:

```python
message = tf.constant('1v1 rust only flashscope')
```

On initialise une session Tensorflow puis on execute le Tenseur avec la m√©thode ```run``` de notre ```tf.Session```:

```python
session = tf.Session()
result = session.run(message).decode()
print(result)
```

> Output: 1v1 rust only flashscope

Si vous lisez la documentation Tensorflow sur la session, vous remarquerez surement qu'il est possible de donner un graph en param√®tre or nous n'en avons pas fourni ici.
Cela s'explique car par d√©fault tout les tenseurs qui sont cr√©es sont automatiquement stock√©s dans le graph "Global", si aucuns param√®tres n'est fourni √† ```tf.Session``` celle ci charge le graph Global.

On aurait aussi pu d√©finir un graph qui n'est pas global:

```python
mock_graph = tf.Graph() 

with mock_graph.as_default() as g:
	message = tf.constant('1v1 rust only flashscope')
	
session = tf.Session(graph=mock_graph)

print(session.run(mock_graph).decode())

tf.reset_default_graph()
```

Ici on utilise ```tf.Graph()``` pour cr√©er un nouveau graph Tensorflow.  
On dit ensuite √† Tensorflow que le nouveau graph consid√©r√© par default est le graph que nous venons de cr√©er, ainsi tout les tenseurs qui seront cr√©es par la suite seront automatiquement ajout√©s √† ce graph:

```python
with mock_graph.as_default() as g:
	... code here ...
```

Enfin la derni√®re √©tape est ```tf.reset_default_graph()```qui permet de dire √† Tensorflow que le nouveau graph par d√©fault est le graph Global.

## Comprendre les graphs Tensorflow

Nous aimerons reproduire le graph de calcul suivant:

![graph ex01](https://raw.githubusercontent.com/PoCFrance/Pool2019/master/ai//images/d03/ex01.png)

En outre cela revient √† reproduire l'√©quation $x = 2^2 + 3^2$:

Pour cela nous allons cr√©er 2 Variables (```tf.Variable```), ces variables sont des valeurs qui peuvent √™tre uniquement modifi√©es par la session Tensorflow, elles ne peuvent pas √™tre modifi√©es √† partir de l'exterieur.

```python
import tensorflow as tf

var1 = tf.Variable(2)
var2 = tf.Variable(3)

var1_add = var1 * var1
var2_add = var2 * var2

result = var1_add + var2_add

sess = tf.Session()

print('Result of (2*2) + (3*3): ',  sess.run(result))

```
> Output:
> Error

Ce code ne fonctionnera pas, contraitement au ```tf.constant```, la session n'initialise pas automatiquement les ```tf.Variable```, il faut lancer l'initializer de chaque tenseurs.

```python3
sess.run(var1.initializer)
sess.run(var2.initializer)

print('Result of (2*2) + (3*3): ',  sess.run(result))
```
> Output:
> 13

Lorsque votre projet prend de l'ampleur il devient vite embetant de devoir appeler tout les initializers un part un vous pouvez donc faire appel √† ```tf.global_variables_initializer()```

```python3
sess.run(tf.global_variables_initializer())
print('Result of (2*2) + (3*3): ',  sess.run(result))
```

### Exercice 01

Dans un fichier **ex01.py**  
Reproduisez le graph de calcul Tensorflow qui repr√©sente l'√©quation: $$x = (3*2^2) - 12^5$$

## La notion de placeholder

Jusqu'a pr√©sent nous pouvions uniquement mod√©liser des √©quations mais nous aimerions pouvoir mod√©liser des fonctions prenant des param√®tres qui peuvent √™tre chang√©s √† chaque execution de tenseurs.

Nous allons repr√©senter une fonction tr√®s basique: $$f(x) = 3x + 2$$

```python
import tensorflow as tf

# Tout les tenseurs peuvent √™tre nomm√©s, prenez l'habitude de le faire
x = tf.placeholder(tf.int32, name='x')
result = 3 + x + 2
```

Un ```tf.placeholder``` permet de cr√©er un tenseur qui va contenir un futur param√®tre, comme son nom l'indique voyez un placeholder comme "une boite qui contient une valeur"

Les valeurs de nos placeholders doivent √™tre donn√©s √† l'execution du tenseur en utilisant le deuxi√®me param√®tre de ```tf.Session().run()``` nomm√© **feed_dict**.

Vous remarquerez aussi qu'aucune variables n'ont √©t√© cr√©ees pour $3$ et $2$, c'est parce que Tensorflow detecte qu'il s'agit de constantes et d√©clare automatiquement des ```tf.constant```

Maintenant que nous avons mod√©lis√© cette fonction, nous pouvons l'executer plusieurs fois avec des valeurs diff√©rentes sans avoir √† cr√©er une nouvelle ``tf.Session()``` √† chaque fois.

```python

... suite du code √ßi dessus ...

sess = tf.Session()

print(sess.run(result, feed_dict={x: 3}))
print(sess.run(result, feed_dict={x: 10}))
print(sess.run(result, feed_dict={x: 100}))

```

> Output:
> 8
> 15
> 105

Notez cependant que Tensorflow cr√©er des d√©pendances entre vos tenseurs, par exemple prenons cette fonction: $$ f(x, y) = 3x + y $$

```python3
import tensorflow as tf

x = tf.placeholder(tf.int32, name='x')
y = tf.placeholder(tf.int32, name='y')

result = 3*x+y

sess = tf.Session()
```

On essaye maintenant d'executer notre fonction en donnant seulement $x$:

```python
print(sess.run(result, feed_dict={x: 3}))
```

![nuclear](https://media.giphy.com/media/HhTXt43pk1I1W/200.gif)

Cela vient du fait que le tenseur $result$ a une d√©pendance sur $x$ mais √©galement une d√©pendance sur $y$, ainsi il faut donner $x$ et $y$ dans notre feed dict pour que cela fonctionne:

```python
print(sess.run(result, feed_dict={x: 3, y: 2}))
```

> Output:
> 11

## La puissance des tenseurs

Reprenons la derni√®re fonction: $$f(x) = 3x + y $$

Jusqu'a pr√©sent, tout les graph que nous avons cr√©ers ne pouvaient prendre en param√®tres que des scalaires (nombres).

Comme vu dans la deuxi√®me journ√©e, pour mod√©liser un r√©seau de neurones nous aimerions pouvoir tout repr√©senter sous forme vectoriel.  

Tensorflow propose justement de faire les calculs sous forme vectoriel !
Pour cela il faut pr√©ciser la dimension de la donn√©e qu'on souhaite manipuler (shape en anglais), reprenons l'exemple √ßi dessous:

```python3
import tensorflow as tf

x = tf.placeholder(tf.int32, name='x')
y = tf.placeholder(tf.int32, name='y')

result = 3*x+y

sess = tf.Session()
```

Nous aimerions transformer ce code afin que $x$ et $y$ soit des vecteurs de dimension 5:

```python3
import tensorflow as tf

x = tf.placeholder(tf.int32, name='x', shape=[5])
y = tf.placeholder(tf.int32, name='y', shape=[5])

result = 3*x+y

sess = tf.Session()
```

Et voil√† ! Notre graph est d√©ja pr√™t √† prendre en param√®tres 2 vecteurs.
Il est important de noter que Tensorflow travaille avec la librairie Numpy, les param√®tres $x$ et $y$ devront √™tre de type Numpy.Array

Essayons de lancer quelque exemples:

```python
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.array([5, 4, 3, 2, 1])
print(sess.run(result, feed_dict={x: arr1, y: arr2}))
```
> Output:
> [8 10 12 14 16]

```python
arr1 = np.random.uniform(size=[5])
arr2 = np.random.uniform(size=[5])
print(sess.run(result, feed_dict={x: arr1, y: arr2}))
```

> Output:
> Random

## Les optimiseurs

Faire une descente de gradient √† la main peut rapidement devenir r√©p√©titif et tr√®s vite compliqu√© √† maintenir, Tensorflow propose donc des optimiseurs d√©ja pr√™t √† minimiser vos fonctions !

Prenons la m√™me √©quation que pr√©c√©dement:

$$ x = Œ∏‚ÇÅ + Œ∏‚ÇÇ + 2 $$

Nous aimerions trouver les valeurs de Œ∏‚ÇÅ et Œ∏‚ÇÇ qui minimise l'√©quation.

On instancie d√©ja notre √©quation:

```python
import tensorflow as tf

theta01 = tf.Variable(2)
theta02 = tf.Variable(10)
result = theta01 + theta02 + 2

sess = tf.Session()
sess.run(result)

```
> Output: 14

Puis on d√©clare un optimiseur (Descente de Gradient i√ßi), on lui demande de minimiser l'output de notre equation

```
import tensorflow as tf

theta01 = tf.Variable(2)
theta02 = tf.Variable(10)
result = theta01 + theta02 + 2

sess = tf.Session()
optimizer = tf.train.GradientDescentOptimizer(0.1)
train_op = optimizer.minimize(result)
```

Ici nous avons dans un premier temps instanci√© un optimiseur, puis cr√©e un tenseur d'optimisation, √† chaque fois que ce tenseur est appel√©, une √©tape de descente de gradient sera effectu√©e sur les param√®tres de notre √©quation.
Il suffit de mettre cela dans une boucle pour voir le r√©sultat:

```
import tensorflow as tf

theta01 = tf.Variable(2.0)
theta02 = tf.Variable(10.0)
result = theta01 + theta02 + 2.0

sess = tf.Session()
sess.run(tf.global_variables_initializer())
optimizer = tf.train.GradientDescentOptimizer(0.1)
train_op = optimizer.minimize(result)

for global_step in range(50):
	out, _ = sess.run([result, train_op])
	print(out)
```

Lancez ce code, vous verrez le r√©sultat de notre expression diminue ! (Remarquez que les int32 on √©t√© pass√© en float32, les optimizers ne peuvent pas travailler avec des Int)

Comme vous pouvez le voir le r√©sultat de notre √©quation diminue de plus en plus car nous la minimisons en fonction de $Œ∏‚ÇÅ$ et $Œ∏‚ÇÇ$

Un autre point important, en param√®tre de **session.run()** nous avons donn√© une liste, cela permet d'executer plusieurs Tenseurs en un seul parcours de Graph.

## Faire le point

A l'heure actuelle vous savez:
	
* Mod√©liser une fonction dans Tensorflow
* Cr√©er des constantes, des variables et des placeholders
* Minimiser une fonction

Prenez une pause, la journ√©e commence maintenant üôÉ

---

### Exercice 02

Dans un fichier **ex02.py**, commencez par importer Tensorflow et Numpy

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
```

Le but de l'exercice est d'effectuer une regression lin√©aire sur le prix des maisons √† Boston.

Commencez par r√©cuperer le dataset:

```python
boston = tf.contrib.learn.datasets.load_dataset('boston')
X_train, Y_train = boston.data[:, 5], boston.target
```

X_train et Y_train sont vos donn√©es d'entrainement pour cet exercice:

* **X_train**: nombre de chambres
* **Y_train**: prix de la maison

Vous pouvez afficher les donn√©es en utilisant matplotlib

```python
plt.scatter(X_train, Y_train)
plt.show()
```

Pensez toujours √† prendre le temps d'explorer vos donn√©es d'entrainement, c'est vraiment important !

![plot ex02](https://raw.githubusercontent.com/PoCFrance/Pool2019/master/ai//images/d03/ex02_plot.png)

Il ne vous reste plus qu'a constituer le graph tensorflow et √† entrainer votre mod√®le.
Si vous avez un doute n'h√©sitez pas √† aller sur la documentation de Tensorflow, elle est tr√®s clair.

Pour la fonction d'erreur vous pouvez la d√©finir vous m√™me ou utiliser la [MSE](https://www.tensorflow.org/api_docs/python/tf/losses/mean_squared_error) de Tensorflow

Voici le genre de r√©sultat √† obtenir:

![result ex02](https://raw.githubusercontent.com/PoCFrance/Pool2019/master/ai//images/d03/ex02_result.png)

---

TIPS:

Vous pouvez d√©clarer une variable d'une valeur al√©atoire en utilisant [```tf.random_uniform```](https://www.tensorflow.org/api_docs/python/tf/random/uniform). 
La shape pr√©cis√©e a ```tf.random_uniform``` est (), elle repr√©sente une dimension de 1, en gros juste un nombre (scalaire).

```python
var = tf.Variable(tf.random_uniform(()))
```

---

Selon vous comment pourriez vous faire pour augmenter la vitesse d'entrainement et la pr√©cision ?
Vous pouvez appeler un assistant pour donner votre opinion.

---

### Exercice 03

Il arrive souvent que notre r√©sultat soit d√©pendant de plusieurs variables d'entr√©es, en r√©alit√© il est m√™me tr√®s rare que le dataset soit d√©pendant que d'une seule variable.

Dans cet exercice nous utilisons le m√™me dataset que l'exercice pr√©cent sauf que nous utilisons toutes les variables afin d'effectuer une [regression lineaire √† plusieurs variables](https://www.hackerearth.com/fr/practice/machine-learning/linear-regression/multivariate-linear-regression-1/tutorial/).

Dans un fichier **ex03.py** commencez par importer les packages habituels:

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
```

Puis chargez le dataset complet:

```python
boston = tf.contrib.learn.datasets.load_dataset('boston')
X_train, Y_train = boston.data, boston.target
```

Pensez toujours √† afficher les dimensions des donn√©es avec lesquels vous travaillez, car cela vous permet de d√©finir vos placeholders, variables, etc...

Vous pourriez avoir besoin d'utiliser [```tf.reduce_mean```](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean)

---

## Sauvegarder un mod√®le

Vous vous √™tes surement pos√© la question de comment il est possible de garder l'√©tat de notre mod√®le apr√®s l'entrainement, car jusqu'√† pr√©sent, une fois le script ferm√©, tout notre entrainement √©tait perdu.

Tensorflow a pens√© √† une solution nomm√©e ```tf.Saver``` c'est une solution tr√®s simple pour sauvegarder un mod√®le dans un √©tat donn√©.

Pour sauvegarder:

```python
import tensorflow as tf

... mod√®le d√©fini et entrain√© ...

saver = tf.Saver()

saver.save('path/de/sauvegarde', sess)
```

Pour restaurer:

```python
import tensorflow as tf

... mod√®le d√©fini et entrain√© ...

saver = tf.Saver()

saver.restore(sess, 'path/de/sauvegarde')
```

Lors d'une restauration de mod√®le il suffit de d√©finir une session mais pas besoin d'initialiser les variables, c'est le saver qui s'occupe de restaurer les valeurs de chaque param√®tres.

---

## Tensorboard ##

Tensorflow propose un outil tr√®s puissant pour la visualisation de donn√©es, il se nomme Tensorboard.

Avec Tensorboard vous pouvez afficher √©norm√©ment d'informations de nature diff√©rentes (images, scalaire, embedding, etc...).  
Pour cette piscine nous utiliserons uniquement l'affichage d'Images et de scalaires.


Pour utiliser Tensorboard il faut commencer par instancier un filewriter en donnant l'attribut 
```graph``` de notre ```tf.Session```:


```python
writer = tf.summary.FileWriter('path/vers/vos/logs', sess.graph)
```

Il faut ensuite cr√©er des Tenseurs qui s√©rialise les tenseurs que l'ont souhaitent afficher dans tensorboard

```python
# On souhaite afficher la variable 'var1' qui est un scalaire
var1 = tf.Variable(2.7)

# On cr√©er ensuite un tenseur de s√©rialisation
var1_summary = tf.summary.scalar(var1, name='variable1')

# Puis on l'ajoute a Tensorboard grace au FileWriter
filewriter.add_summary(idx, var1_summary)

```
**idx** repr√©sente le timestamp, essayer de le changer vous comprendrez l'inter√™t

Il ne vous reste plus qu'a lanc√© tensorboard de cette mani√®re:

```bash
tensorboard --logdir='path/vers/vos/logs'
```

Et de visiter avec votre navigateur **127.0.0.1:6006**


### Exercice 04

Cet exercice est un peu comme le Hello World du deep learning, vous allez cr√©er un mod√®le capable de reconnaitre des chiffres manuscrits !

L'exercice est compos√© de deux parties, une partie en utilisant **l'API bas niveau** de Tensorflow et une deuxi√®me partie beaucoup plus courte ou nous aborderons **l'API haut niveau** de Tensorflow qui nous accompagnera durant tout le reste de la piscine.

Faites un r√©seau de neuronne comportant 784 neurones d'entr√©es, 300 neurones cach√©es et 10 neurones de sorties qui d√©signent une probablit√©s pour chaque chiffres.

Posez vous la question de pourquoi 784 neurones en entr√©e, regardez bien les dimensions du dataset.

Le dernier layer de ce r√©seau d√©signe des probabilit√©s, pour appliquer une non lin√©arit√© sur ce layer on utilise la fonction d'activation [**softmax**](https://fr.wikipedia.org/wiki/Fonction_softmax), c'est une fonction tr√®s utilis√©e dans les probl√®mes de classification, on la met sur le dernier layer uniquement car elle fixe les bornes de notre vecteur de sortie entre 0 et 1 (0 √† 100%), elle est aussi beaucoup plus longue √† calculer que la fonction sigmoid ([```tf.nn.softmax```](https://www.tensorflow.org/api_docs/python/tf/nn/softmax))

A titre d'exemple, voici ce que la fonction softmax produit comme r√©sultat:


$$ z = (1;3;2,5;5;4;2) $$  

$$ softmax(z) = (0,011;0,082;0,050;0,605;0,222;0,030) $$

Vous pouvez appliquer la cross entropie, elle fonctionne √©galement tr√®s bien avec la softmax, afin de vous faciliter la tache renseignez vous sur la [sparse softmax cross entropy](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)

‚ö† Pensez bien √† regarder la formule appliqu√©e par Tensorflow lors de l'utilisation de la sparse softmax cross entropy, appliquer 2 fois une non lin√©arit√© rend instable votre mod√®le.

Dans un fichier **ex04.png** commencez par importer:

```python
import tensorflow as tf
import numpy as np
```

Puis charger le MNIST Dataset:

```python
dataset = tf.keras.datasets.mnist.load_data(path='mnist.npz')
```

**TIPS**:
Vous pouvez cr√©er une variable d'une shape sp√©cifique, voi√ßi un exemple:

```python
W = tf.Variable(tf.random_uniform([300, 1])
```

![mnist](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

---

### Exercice 05

Comme vous l'aurez remarqu√© pendant le dernier exercice, cr√©er un r√©seau de neuronne avec l'API bas niveau de Tensorflow devient vite tr√®s long et r√©p√©titif, pour la suite de la piscine nous allons utiliser une API tr√®s haut niveau de Tensorflow, Keras.

L'installation de Keras ce fait ainsi:


```python
pip3 install keras
```

Je vous invite √† lire le [guide de d√©marrage Keras](https://keras.io/getting-started/sequential-model-guide/)

Vous pouvez utiliser l'API fonctionelle ou sequentielle de Keras, prenez celle avec laquelle vous semblez le plus √† l'aise.

TIPS:

- Vous aurez besoin des Dense layers

---

### Exercice 06

Reprenez le code pr√©cedement cr√©e et essayer le avec ce nouveau dataset:

```python
tf.keras.datasets.fashion_mnist.load_data()
```

La dimension des donn√©es est la m√™me, seule l'organisation du dataset change.

A votre avis, pourquoi la pr√©cision est m√©diocre ?

## Conclusion de la journ√©e

Cette journ√©e avait pour but de vous apprendre √† r√©soudre les probl√®mes vous m√™me avec une nouvelle technologie, aujourd'hui nous utilisons Tensorflow, peut √™tre que demain vous utiliserez plutot PyTorch, vous devez √™tre capable de vous adapter √† plusieurs milleux diff√©rents. La data science n'√©chappe pas √† cette mentalit√©, un bon data scientist est quelqu'un qui prend le temps de lire les nouveaux papiers de recherche et apprend comment les appliquer en production.

Enfin, vous avez vu que les r√©seaux de neurones ont leurs limite sur des donn√©es complexes, nous verrons demain comment rem√©dier √† cela.






